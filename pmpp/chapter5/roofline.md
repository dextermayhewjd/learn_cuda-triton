ä¸€ã€GPU ä¼˜åŒ–çš„æ€»çº²ï¼ˆå…ˆç»™ç»“è®ºï¼‰
GPU ä¼˜åŒ– = æ‰¾ç“¶é¢ˆ â†’ é’ˆå¯¹ç“¶é¢ˆé‚£ä¸€å±‚â€œå¯¹ç—‡ä¸‹è¯â€

## ç¬¬ä¸€å±‚ï¼šå†…å­˜è®¿é—®ä¼˜åŒ–ï¼ˆMemory-bound æ—¶ï¼‰

è¿™æ˜¯ä½ æåˆ°çš„é‡ç‚¹ï¼Œä¹Ÿæ˜¯æœ€å¸¸è§ç“¶é¢ˆã€‚

### 1ï¸âƒ£ Coalesced è®¿é—®ï¼ˆè¿™æ˜¯åº•çº¿ï¼Œä¸æ˜¯ä¼˜åŒ–ï¼‰
æ ¸å¿ƒè§„åˆ™ï¼ˆä¸€å®šè¦ä¼šèƒŒï¼‰

ä¸€ä¸ª warpï¼ˆ32 threadsï¼‰
è®¿é—®è¿ç»­ã€å¯¹é½çš„å†…å­˜åœ°å€

```c++
// å¥½
addr = base + threadIdx.x
// å
addr = base + threadIdx.x * stride
```

åˆ¤æ–­æ–¹æ³•  
ä¸€ä¸ª warp æ˜¯å¦èƒ½ç”¨ 1â€“2 æ¬¡ memory transaction  
è€Œä¸æ˜¯ 32 æ¬¡  
ğŸ“Œ ä¸ coalescedï¼Œåé¢ä¸€åˆ‡ä¼˜åŒ–ç™½æ­  

### 2ï¸âƒ£ Cache åˆ©ç”¨ï¼ˆL2 / L1 / Read-only cacheï¼‰
ä½ ä¸ä¸€å®šæ€»ç”¨ shared memoryã€‚  
å¸¸è§æ‰‹æ®µ  
æ•°æ®å¤ç”¨  
åŒä¸€ä¸ªæ•°æ®è¢«å¤šä¸ª warp / å¤šæ¬¡ä½¿ç”¨  
blocking / tiling  
__ldg()ï¼ˆè€æ¶æ„ï¼‰æˆ–åªè¯» cache è·¯å¾„  
åˆç†çš„æ•°æ® layoutï¼ˆAoS â†’ SoAï¼‰  
ğŸ“Œ å¾ˆå¤š kernelï¼š  
ä¸æ˜¯ bandwidth ä¸å¤Ÿï¼Œè€Œæ˜¯ cache miss å¤ªå¤š  
### 3ï¸âƒ£ Shared Memoryï¼ˆä½ æåˆ°çš„ï¼‰
shared memory çš„æ­£ç¡®å®šä½æ˜¯ï¼š  
ç”¨ç©ºé—´æ¢å¸¦å®½ï¼Œç”¨åŒæ­¥æ¢å¤ç”¨  

#### ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ

åŒä¸€æ•°æ®è¢«å¤šä¸ª thread å¤šæ¬¡ç”¨  
global â†’ shared â†’ å¤šæ¬¡ compute  
#### å¸¸è§ç”¨é€”  
GEMM tiling  
attention ä¸­çš„ K/V tile  
stencil / convolution  
#### ä½†æ³¨æ„ä¸¤ä¸ªå‘  
âŒ Bank conflict  
è¿ç»­ thread è®¿é—®åŒä¸€ä¸ª bank â†’ ä¸²è¡Œ  
âŒ ç”¨äº† shared ä½† occupancy æ‰å…‰  
shared ç”¨å¤ªå¤š â†’ block æ•°ä¸‹é™  
ğŸ‘‰ shared æ˜¯æ‰‹æœ¯åˆ€ï¼Œä¸æ˜¯å¤§é”¤  
### 4ï¸âƒ£ é¢„å–ï¼ˆLatency hiding çš„å…³é”®ï¼‰

```c++
// pseudo  
load next_tile  
compute current_tile  
software pipelining  
double buffer  
cp.asyncï¼ˆAmpere+ï¼‰  
```

ğŸ“Œ GPU ä¸æ€• latencyï¼Œåªæ€•æ²¡äº‹å¹²

## ç¬¬äºŒå±‚ï¼šå¹¶è¡Œåº¦ & Occupancyï¼ˆMemory / Compute éƒ½éœ€è¦ï¼‰

å³ä½¿ memory-boundï¼Œä½ ä¹Ÿéœ€è¦è¶³å¤Ÿå¤šçš„ warpã€‚

### 1ï¸âƒ£ Occupancy ä¸æ˜¯è¶Šé«˜è¶Šå¥½ï¼Œä½†ä¸èƒ½å¤ªä½

å†³å®šå› ç´   
registers / thread  
shared memory / block  
block size  
å¸¸è§è¯¯åŒº  
â€œoccupancy 100% ä¸€å®šæœ€å¿«â€ âŒ  
ç°å®æ˜¯ï¼š  
50â€“70% å¸¸è®°å·²ç»å¤Ÿéšè— latency  

### 2ï¸âƒ£ Warp-level æ€ç»´ï¼ˆéå¸¸é‡è¦ï¼‰

ä½ è¦å¼€å§‹ä»¥ warp ä¸ºå•ä½æ€è€ƒ  
warp divergenceï¼ˆif/elseï¼‰  
warp-level primitiveï¼ˆ__shfl, __syncwarpï¼‰  
warp-reduction ä»£æ›¿ block-reduction  
ğŸ“Œ å¾ˆå¤š kernelï¼š
æ…¢åœ¨ warp divergenceï¼Œä¸æ˜¯ FLOP  
## ç¬¬ä¸‰å±‚ï¼šæŒ‡ä»¤ & è®¡ç®—ä¼˜åŒ–ï¼ˆCompute-bound æ—¶ï¼‰

å½“ä½ è¿›å…¥ roofline å³è¾¹ï¼Œå°±è½®åˆ°è¿™äº›ã€‚

### 1ï¸âƒ£ ç”¨åˆ°â€œå¯¹çš„è®¡ç®—å•å…ƒâ€

è¿™æ˜¯ AI infra æœ€æ ¸å¿ƒçš„ä¸€ç‚¹ã€‚

å†™æ³•	ç”¨çš„ç¡¬ä»¶
æ™®é€š FP32	CUDA core  
FP16/BF16 Tensor Core	HMMA  
INT8	IMMA  

ğŸ‘‰ Roofline çš„ peak GFLOPS
é€šå¸¸å‡è®¾ä½ åœ¨ç”¨ Tensor Core

### 2ï¸âƒ£ æé«˜ ILPï¼ˆInstruction-Level Parallelismï¼‰ 

è®© GPU scheduler æœ‰ä¸œè¥¿å¯é€‰ï¼š  
unrollï¼ˆé€‚åº¦ï¼‰   
å‡å°‘ dependency chain    
å¤š accumulator  
acc0 += a0 * b0;  
acc1 += a1 * b1;  
ğŸ“Œ ä¸è¦è®©æŒ‡ä»¤ä¸€æ¡æ¡â€œæ’é˜Ÿç­‰å‰ä¸€ä¸ªâ€

### 3ï¸âƒ£ å‡å°‘â€œé FLOP æŒ‡ä»¤â€

Roofline åªæ•° FLOPï¼Œä½† GPU ä¸è¿™ä¹ˆæƒ³ã€‚

è¦è­¦æƒ•ï¼š

address calculation

integer math

type cast

atomic

sync

ğŸ‘‰ æœ‰æ—¶ä½  FLOP å¾ˆé«˜ï¼Œä½†ï¼š

æ¯ä¸ª FLOP éƒ½å¤¹ç€ä¸€å †æ‚æ´»

## ç¬¬å››å±‚ï¼šKernel å½¢æ€ & ç²’åº¦
### 1ï¸âƒ£ Kernel å¤ªå° = GPU æ²¡çƒ­èµ·æ¥

é—®é¢˜è¡¨ç°ï¼š

launch overhead æ˜¾è‘—

SM åˆ©ç”¨ç‡ä½

è§£å†³ï¼š

fuse kernels

batch

persistent kernel

### 2ï¸âƒ£ Kernel fusionï¼ˆAI infra éå¸¸é‡è¦ï¼‰

ä¾‹å¦‚ï¼š

bias + activation

attention çš„å¤šä¸ª stage åˆå¹¶

ğŸ“Œ å‡å°‘ global memory round trip = æé«˜è®¡ç®—å¼ºåº¦

å…­ã€æŠŠè¿™äº›æ˜ å°„å› Rooflineï¼ˆéå¸¸å…³é”®ï¼‰

ä½ å¯ä»¥è¿™æ ·åš è¯Šæ–­ â†’ è¡ŒåŠ¨ï¼š

ğŸ” æƒ…å†µ Aï¼šç‚¹åœ¨æ–œçº¿ä¸‹æ–¹ï¼ˆmemory-boundï¼‰

é—®è‡ªå·±ï¼š

coalesced äº†å—ï¼Ÿ

cache hit é«˜å—ï¼Ÿ

shared / tiling æœ‰æ„ä¹‰å—ï¼Ÿ

warp æ•°å¤Ÿå—ï¼Ÿ

ğŸ‘‰ è¡ŒåŠ¨ï¼šè®¿å­˜ & å¹¶å‘ä¼˜åŒ–

ğŸ” æƒ…å†µ Bï¼šç‚¹åœ¨å³è¾¹ä½†ç¦»å±‹é¡¶è¿œï¼ˆcompute-bound ä½†ä¸æ»¡ï¼‰

é—®è‡ªå·±ï¼š

ç”¨ Tensor Core äº†å—ï¼Ÿ

ç²¾åº¦å¯¹äº†å—ï¼Ÿ

æŒ‡ä»¤ dependency å¤šå—ï¼Ÿ

kernel å¤ªå°äº†å—ï¼Ÿ

ğŸ‘‰ è¡ŒåŠ¨ï¼šç®—åŠ›è·¯å¾„ & ILP

ä¸ƒã€ç»™ä½ ä¸€ä¸ªâ€œGPU ä¼˜åŒ–æµç¨‹è¡¨â€ï¼ˆä½ å¯ä»¥ç…§ç€ç”¨ï¼‰
1. ç”» roofline / ç®— intensity  
2. å®šä½ memory-bound è¿˜æ˜¯ compute-bound
3. memory-bound:
   - coalescing
   - cache
   - shared / tiling
   - occupancy
4. compute-bound:
   - Tensor Core
   - ILP
   - precision
5. çœ‹ kernel ç²’åº¦ & fusion